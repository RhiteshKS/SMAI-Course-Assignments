{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: MLP\n"
      ],
      "metadata": {
        "id": "2hy9O38GJs5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J20q9POPJvR3",
        "outputId": "7c0ca088-f824-43fd-b609-e68d12084b39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJzk0zP1j8ET",
        "outputId": "0c8846b7-505e-44d5-dfb6-9cb84441aa99"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32 * 3)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "mlp = MLP().to(device)\n"
      ],
      "metadata": {
        "id": "jMkMJFOOLFBq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(mlp.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(5):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "\n",
        "print('Finished Training MLP')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oAJSkOuxX_T",
        "outputId": "3eb67c6e-2eac-4486-8fd9-8e095b6189d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 1.935\n",
            "[1,  4000] loss: 1.717\n",
            "[1,  6000] loss: 1.663\n",
            "[1,  8000] loss: 1.617\n",
            "[1, 10000] loss: 1.588\n",
            "[1, 12000] loss: 1.557\n",
            "[2,  2000] loss: 1.477\n",
            "[2,  4000] loss: 1.492\n",
            "[2,  6000] loss: 1.461\n",
            "[2,  8000] loss: 1.463\n",
            "[2, 10000] loss: 1.465\n",
            "[2, 12000] loss: 1.446\n",
            "[3,  2000] loss: 1.361\n",
            "[3,  4000] loss: 1.378\n",
            "[3,  6000] loss: 1.347\n",
            "[3,  8000] loss: 1.362\n",
            "[3, 10000] loss: 1.381\n",
            "[3, 12000] loss: 1.389\n",
            "[4,  2000] loss: 1.287\n",
            "[4,  4000] loss: 1.297\n",
            "[4,  6000] loss: 1.310\n",
            "[4,  8000] loss: 1.301\n",
            "[4, 10000] loss: 1.306\n",
            "[4, 12000] loss: 1.307\n",
            "[5,  2000] loss: 1.226\n",
            "[5,  4000] loss: 1.257\n",
            "[5,  6000] loss: 1.247\n",
            "[5,  8000] loss: 1.244\n",
            "[5, 10000] loss: 1.261\n",
            "[5, 12000] loss: 1.269\n",
            "Finished Training MLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = mlp(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "print(f'Average test loss: {test_loss / len(testloader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAXJ3gdskh0i",
        "outputId": "e44194cd-353d-4459-9346-02a3fbea39ef"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 50 %\n",
            "Average test loss: 1.415122298401594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MLP model achieves an accuracy of 50% on the CIFAR-10 test set with average test loss of 1.415, it means that it correctly classified half of the images in the test set.\n",
        "\n",
        "An accuracy of 49% is relatively low for the CIFAR-10 dataset. The CIFAR-10 dataset is challenging due to its small image size (32x32 pixels) and diverse range of object categories. Achieving higher accuracies usually requires more sophisticated models or techniques."
      ],
      "metadata": {
        "id": "4DwMBeS4xpp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: CNN"
      ],
      "metadata": {
        "id": "uZQcCyIvNB3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "metadata": {
        "id": "bf9TuGDXOCgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4695018-feee-43bc-e719-4ebbea41a4a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 87606887.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "cnn = SimpleCNN().to(device)\n"
      ],
      "metadata": {
        "id": "vtcvctkmwRFD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "nsvDVZxhwUey"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = cnn(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXR2bCZzwYTE",
        "outputId": "ddf492c8-e22b-44f7-f3d4-298e17c3b1f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.204\n",
            "[1,  4000] loss: 1.883\n",
            "[1,  6000] loss: 1.686\n",
            "[1,  8000] loss: 1.595\n",
            "[1, 10000] loss: 1.528\n",
            "[1, 12000] loss: 1.467\n",
            "[2,  2000] loss: 1.377\n",
            "[2,  4000] loss: 1.351\n",
            "[2,  6000] loss: 1.327\n",
            "[2,  8000] loss: 1.306\n",
            "[2, 10000] loss: 1.274\n",
            "[2, 12000] loss: 1.246\n",
            "[3,  2000] loss: 1.169\n",
            "[3,  4000] loss: 1.176\n",
            "[3,  6000] loss: 1.175\n",
            "[3,  8000] loss: 1.166\n",
            "[3, 10000] loss: 1.156\n",
            "[3, 12000] loss: 1.169\n",
            "[4,  2000] loss: 1.054\n",
            "[4,  4000] loss: 1.080\n",
            "[4,  6000] loss: 1.071\n",
            "[4,  8000] loss: 1.064\n",
            "[4, 10000] loss: 1.050\n",
            "[4, 12000] loss: 1.076\n",
            "[5,  2000] loss: 0.994\n",
            "[5,  4000] loss: 1.012\n",
            "[5,  6000] loss: 0.997\n",
            "[5,  8000] loss: 0.978\n",
            "[5, 10000] loss: 1.011\n",
            "[5, 12000] loss: 1.000\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = cnn(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "print(f'Average test loss: {test_loss / len(testloader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mys0jNoLbemd",
        "outputId": "0aee7a10-798c-43dc-c007-cf72b852eebd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 64 %\n",
            "Average test loss: 1.03826994513534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An accuracy of 64% indicates that the CNN model performed better than the MLP model on the CIFAR-10 dataset. CNNs are well-suited for image classification tasks due to their ability to capture spatial hierarchies of features, which might have contributed to the improved performance.\n",
        "\n",
        "Even though the CNN model achieved 64% accuracy with average loss of 1.038, there may still be room for improvement."
      ],
      "metadata": {
        "id": "h9schPk1zM0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: VGG"
      ],
      "metadata": {
        "id": "1DDPU-1_OOnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize(224),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-zudBKydHBo",
        "outputId": "d14865d6-fadf-46b8-8798-94572dda3fd7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "for param in vgg16.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "features = list(vgg16.classifier.children())[:-1]\n",
        "features.extend([nn.Linear(num_features, len(classes))])\n",
        "vgg16.classifier = nn.Sequential(*features)\n",
        "\n",
        "vgg16 = vgg16.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEOtXpesdJ_a",
        "outputId": "aa8c6913-2081-4d2f-9a2c-d661bc16676c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.001, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "TAeCoj_QdMsS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = vgg16(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:\n",
        "            # print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekcUbY7jdPfc",
        "outputId": "62bdc4a6-f22f-41e2-fa8c-13a7f32b4bd5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = vgg16(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n",
        "print(f'Average test loss: {test_loss / len(testloader)}')\n"
      ],
      "metadata": {
        "id": "8b8cL5c6u9UU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d577eb39-9634-471b-ee70-9a67403a9488"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 83.75 %\n",
            "Average test loss: 0.47695332286274356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pretrained VGG16 model achieves an accuracy of 83.75% and average test loss of 0.476 on the CIFAR-10 test set, it demonstrates strong performance and significant improvement over both the MLP and CNN models.\n",
        "\n",
        "This suggests that leveraging transfer learning from a pre-trained model like VGG16, which was originally trained on a large dataset like ImageNet, can lead to superior performance compared to training models from scratch.\n",
        "\n",
        "Transfer learning allows the model to leverage the knowledge learned from a large dataset (ImageNet) to extract meaningful features relevant to the CIFAR-10 classification task. The pretrained VGG16 model has learned to recognize a wide range of low-level and high-level features, which benefits the classification of CIFAR-10 images.\n",
        "\n",
        "The high accuracy achieved by the pretrained VGG16 model suggests strong generalization ability."
      ],
      "metadata": {
        "id": "21FsRe5XziNv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PezMn3LZl9Xx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}